## 1. 들어가며
### 1. 언어 모델 소개
* **언어모델(Language Model)**: 문장의 확률을 나타내는 모델
  * 문장 자체의 출현 예측 ex) P(오늘 점심을 먹었다) > P(3월 2일 오후 1시에 점심을 먹었다)
  * 이전 단어들이 주어졌을 때 다음 단어 예측 ex) 오늘 점심을 먹었다/마셨다/뿌렸다
  
### 2. 한국어 언어모델



## 5. NNLM (Neural Network Language Model)
n-gram 기반 언어 모델의 약점을 보완하는 신경망 언어모델!

### 1. 희소성 해결하기
* n-gram 기반 언어 모델의 약점: 단어간의 유사도를 알지 못함
ex) '고양이는 진짜 귀여워' --> P(귀여워 | 강아지는, 진짜) > P(귀여워 | 물은, 진짜) 임을 알 수 없음       
--> 이를 보완하기 위해, 단어 임베딩을 사용하여 '강아지'와 '고양이'가 높은 유사도를 가지기 때문에 P(귀여워 | 강아지는, 진짜) > P(물은, 진짜) 임을 알아낼 수 있다.        
--> (단어의) 희소성 해소를 통해 더 좋은 일반화 성능을 얻어낼 수 있음

### 2. RNNLM (RNN Language Model)
* 마르코프 가정을 사용하여 확률을 계산하지 않음
* 단어 임베딩을 통해 희소성 문제를 해소하였기 때문에 문장의 첫 단어(w1)부터 해당 단어 직전의 단어(wi-1)까지 모두 조건부에 넣어 확률 근사 가능

$P(w_1, )$
