{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPwithPython1_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4By/HCujUYHXSB57lzZeM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim-ji-youn/Study-with-NLP-books/blob/main/1.%20Mastering%20Natural%20Language%20Processing%20with%20Python/1.%20NLPwithString/2.%20TextNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmoZl6KplHvq",
        "outputId": "a0816bec-847b-431d-b33e-ccef6de52edb"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5cZOLlDkfi8"
      },
      "source": [
        "# Regular Expression 정규화\r\n",
        "  1. 문장 부호 제거\r\n",
        "  2. 전체 텍스트의 소문자 혹은 대문자 변환\r\n",
        "  3. 불용어(stopwords) 제거\r\n",
        "  4. 텍스트 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Actv7MfjkwGS"
      },
      "source": [
        "## 1. 문장 부호 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IU_r-hikTJ6",
        "outputId": "cae1883f-bd9f-4da0-e169-12cfe4294e94"
      },
      "source": [
        "#토큰화\r\n",
        "text = \"\"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\"\"\r\n",
        "\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "doc = sent_tokenize(text)\r\n",
        "print(doc)\r\n",
        "tokenized_doc = [word_tokenize(sentence) for sentence in doc]\r\n",
        "print(tokenized_doc)\r\n",
        "\r\n",
        "#문장 부호 제거\r\n",
        "import re\r\n",
        "import string\r\n",
        "x = re.compile('[%s]' %re.escape(string.punctuation))\r\n",
        "\r\n",
        "tokenized_docs_no_punctuation = []\r\n",
        "for sent in tokenized_doc :\r\n",
        "  new_sent = []\r\n",
        "  for token in sent:\r\n",
        "    new_token = x.sub(u'', token)\r\n",
        "    if not new_token == u'':\r\n",
        "      new_sent.append(new_token)\r\n",
        "  tokenized_docs_no_punctuation.append(new_sent)\r\n",
        "\r\n",
        "print(tokenized_docs_no_punctuation)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Python is an interpreted, high-level and general-purpose programming language.', \"Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']\n",
            "[['Python', 'is', 'an', 'interpreted', ',', 'high-level', 'and', 'general-purpose', 'programming', 'language', '.'], ['Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.'], ['Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']]\n",
            "[['Python', 'is', 'an', 'interpreted', 'highlevel', 'and', 'generalpurpose', 'programming', 'language'], ['Python', 's', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace'], ['Its', 'language', 'constructs', 'and', 'objectoriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'for', 'small', 'and', 'largescale', 'projects']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VToTiJfCZqH"
      },
      "source": [
        "## 2. 소문자와 대문자로 변환\r\n",
        "* ```lower()```\r\n",
        "* ```upper()```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afxBOP4blkcL",
        "outputId": "e9dd57ed-1103-4724-ef81-d8f9a549bff2"
      },
      "source": [
        "text = \"HARDWORK is KEY to SUCCESS\"\r\n",
        "lower = text.lower()\r\n",
        "print(lower)\r\n",
        "upper = text.upper()\r\n",
        "print(upper)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hardwork is key to success\n",
            "HARDWORK IS KEY TO SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHhDvNSFCze8"
      },
      "source": [
        "## 3. 불용어 처리\r\n",
        "* **불용어(Stopwords)**: 문장의 전체적인 의미에 크게 기여하지 않는 단어들\r\n",
        "\r\n",
        "```\r\n",
        "from nltk.corpus import stopwords\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVQ0OdoyCtAY",
        "outputId": "2981d9fd-107b-41ed-e77f-5e863cc04127"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "stops = stopwords.words('english')\r\n",
        "print(stops)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4JZ_ue-DOU1",
        "outputId": "7cdec395-bae3-4b01-a739-7dfff401d9f7"
      },
      "source": [
        "print(stopwords.fileids())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgTosXLfDz7T",
        "outputId": "03a44ef6-709c-42ac-8373-45365f71e9a2"
      },
      "source": [
        "stops_ger = stopwords.words('german')\r\n",
        "print(stops_ger)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugOoF55tDVn0",
        "outputId": "9a0c6701-b12c-4a80-8dc0-08f2e601032b"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "#sentence segmentation\r\n",
        "text = \"Don't hesitate to ask me a question! I will be here for you.\"\r\n",
        "sentences = sent_tokenize(text)\r\n",
        "print(sentences)\r\n",
        "\r\n",
        "#tokenization & 특수문자 제거\r\n",
        "import re\r\n",
        "import string\r\n",
        "x = re.compile('[%s]' %re.escape(string.punctuation))\r\n",
        "\r\n",
        "tokenized_docs_no_punctuation = []\r\n",
        "for sent in sentences :\r\n",
        "  sent = word_tokenize(sent)\r\n",
        "  new_sent = []\r\n",
        "  for token in sent:\r\n",
        "    new_token = x.sub(u'', token)\r\n",
        "    if not new_token == u'':\r\n",
        "      new_sent.append(token)\r\n",
        "  tokenized_docs_no_punctuation.append(new_sent)\r\n",
        "\r\n",
        "print(tokenized_docs_no_punctuation)\r\n",
        "\r\n",
        "# remove stopwords\r\n",
        "tokenized_docs_no_punct_stop = []\r\n",
        "for sent in tokenized_docs_no_punctuation :\r\n",
        "  new_sent = []\r\n",
        "  for word in sent:\r\n",
        "    if word.lower() not in stops:\r\n",
        "      new_sent.append(word)\r\n",
        "  tokenized_docs_no_punct_stop.append(new_sent)\r\n",
        "\r\n",
        "print(tokenized_docs_no_punct_stop)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Don't hesitate to ask me a question!\", 'I will be here for you.']\n",
            "[['Do', \"n't\", 'hesitate', 'to', 'ask', 'me', 'a', 'question'], ['I', 'will', 'be', 'here', 'for', 'you']]\n",
            "[[\"n't\", 'hesitate', 'ask', 'question'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHldEIcZHH-v"
      },
      "source": [
        "## 4. 텍스트 정규화(Normalization)\r\n",
        "* don't -> do not\r\n",
        "\r\n",
        "### 4-1. 정규 표현식을 활용하여 문장 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weNfhNm1EIcs"
      },
      "source": [
        "# 정규 표현식 사용\r\n",
        "import re\r\n",
        "replacement_patterns = [\r\n",
        "(r'won\\'t', 'will not'), \r\n",
        "(r'can\\'t', 'can not'),\r\n",
        "(r'i\\'m', 'i am'),\r\n",
        "(r'ain\\'t', 'is not'),\r\n",
        "(r'(\\w+)\\'ll', '\\g<1> will'),\r\n",
        "(r'(\\w+)n\\'t', '\\g<1> not'),\r\n",
        "(r'(\\w+)\\'ve', '\\g<1> have'),\r\n",
        "(r'(\\w+)\\'s', '\\g<1> is'),\r\n",
        "(r'(\\w+)\\'re', '\\g<1> are'),\r\n",
        "(r'(\\w+)\\'d', '\\g<1> would')\r\n",
        "]\r\n",
        "\r\n",
        "class RegexpReplacer(object) :\r\n",
        "  def __init__(self, patterns = replacement_patterns):\r\n",
        "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\r\n",
        "  \r\n",
        "  def replace(self, text):\r\n",
        "    s = text\r\n",
        "    for (pattern, repl) in self.patterns:\r\n",
        "      (s, count) = re.subn(pattern, repl, s)\r\n",
        "    return s"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oc2BR_3KJma",
        "outputId": "b9c77d6f-d4f1-41be-fc71-dbb9835868e3"
      },
      "source": [
        "# Normalization before tokenization\r\n",
        "text = \"Don't hesitate to ask questions. She must've gone to the market but she didn't go\"\r\n",
        "replacer = RegexpReplacer()\r\n",
        "normalized_sentence = replacer.replace(text)\r\n",
        "\r\n",
        "print(\"normalized sentences:\\t\", normalized_sentence)\r\n",
        "\r\n",
        "# Sentence Segmentation\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "sentences = sent_tokenize(normalized_sentence)\r\n",
        "print(\"sentence segmentation:\\t\", sentences)\r\n",
        "\r\n",
        "#Tokenization\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "tokens = []\r\n",
        "for sent in sentences :\r\n",
        "  tokens.append(word_tokenize(sent))\r\n",
        "\r\n",
        "print(\"tokenization:\\t\", tokens)\r\n",
        "\r\n",
        "# Remove Stopwords\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stops = stopwords.words('english')\r\n",
        "\r\n",
        "new_tokens = []\r\n",
        "for sent in tokens :\r\n",
        "  new_sent = []\r\n",
        "  for token in sent :\r\n",
        "    if token not in stops :\r\n",
        "      new_sent.append(token)\r\n",
        "  new_tokens.append(new_sent)\r\n",
        "\r\n",
        "print(\"remove stopwords:\\t\", new_tokens)\r\n",
        "\r\n",
        "# Remove 특수문자\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "x = re.compile('[%s]' %re.escape(string.punctuation))\r\n",
        "\r\n",
        "new_tokens_no_punct = []\r\n",
        "\r\n",
        "for sent in new_tokens:\r\n",
        "  new_sent = []\r\n",
        "  for token in sent:\r\n",
        "    new_token = x.sub(u'', token)\r\n",
        "    if not new_token == u'':\r\n",
        "      new_sent.append(token)\r\n",
        "  new_tokens_no_punct.append(new_sent)\r\n",
        "\r\n",
        "print(\"no punctuation:\\t\", new_tokens_no_punct)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalized sentences:\t Do not hesitate to ask questions. She must have gone to the market but she did not go\n",
            "sentence segmentation:\t ['Do not hesitate to ask questions.', 'She must have gone to the market but she did not go']\n",
            "tokenization:\t [['Do', 'not', 'hesitate', 'to', 'ask', 'questions', '.'], ['She', 'must', 'have', 'gone', 'to', 'the', 'market', 'but', 'she', 'did', 'not', 'go']]\n",
            "remove stopwords:\t [['Do', 'hesitate', 'ask', 'questions', '.'], ['She', 'must', 'gone', 'market', 'go']]\n",
            "no punctuation:\t [['Do', 'hesitate', 'ask', 'questions'], ['She', 'must', 'gone', 'market', 'go']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d_htSLrN5LU"
      },
      "source": [
        "### 4-2. 정규 표현식과 wordnet을 활용한 반복 문자 삭제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9ibw3gYN4r4"
      },
      "source": [
        "class RepeatReplacer(object):\r\n",
        "   def __init__(self):\r\n",
        "      self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\r\n",
        "      self.repl = r'\\1\\2\\3'\r\n",
        "\r\n",
        "   def replace(self, word):\r\n",
        "      repl_word = self.repeat_regexp.sub(self.repl, word)\r\n",
        "      if repl_word != word:\r\n",
        "         return self.replace(repl_word)\r\n",
        "      else:\r\n",
        "         return repl_word"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VinqgPLELCHa",
        "outputId": "3cf2b2b8-f4d4-4c7a-e560-e8f7d6154894"
      },
      "source": [
        "replacer = RepeatReplacer()\r\n",
        "print(replacer.replace('lottttt'))\r\n",
        "print(replacer.replace('oohhhhhh'))\r\n",
        "print(replacer.replace('I loveeee itttttt'))\r\n",
        "print(replacer.replace('happy')) # <- 오류: wordnet을 embed한다"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lot\n",
            "oh\n",
            "I love it\n",
            "hapy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0aEpR0jSqfQ"
      },
      "source": [
        "import re\r\n",
        "from nltk.corpus import wordnet\r\n",
        "\r\n",
        "class RepeatReplacer(object):\r\n",
        "  def __init__(self):\r\n",
        "    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\r\n",
        "    self.repl = r'\\1\\2\\3'\r\n",
        "  \r\n",
        "  def replace(self, word):\r\n",
        "    if wordnet.synsets(word):\r\n",
        "      return word\r\n",
        "    repl_word = self.repeat_regexp.sub(self.repl, word)\r\n",
        "    if repl_word != word:\r\n",
        "      return self.replace(repl_word)\r\n",
        "    else:\r\n",
        "      return repl_word"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHrFkhtPTE4d",
        "outputId": "8703e604-785e-4ea0-9b9a-eabc456fd8c9"
      },
      "source": [
        "replacer = RepeatReplacer()\r\n",
        "print(replacer.replace('lottttt'))\r\n",
        "print(replacer.replace('oohhhhhh'))\r\n",
        "print(replacer.replace('I loveeee itttttt'))\r\n",
        "print(replacer.replace('happy')) # <- 오류 수정됨!"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lot\n",
            "ooh\n",
            "I love it\n",
            "happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4SeXQ94UHDV"
      },
      "source": [
        "### 4-3. 단어를 동의어로 대체하는 함수 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFstiZVtTmGG"
      },
      "source": [
        "class WordReplacer(object):\r\n",
        "  def __init__(self, word_map):\r\n",
        "    self.word_map = word_map\r\n",
        "  \r\n",
        "  def replace(self, word):\r\n",
        "    return self.word_map.get(word, word)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BUi99agjUaPJ",
        "outputId": "c4addedc-37a8-4267-a6c1-7a183e4d9441"
      },
      "source": [
        "replacer = WordReplacer({'congrats': 'congratulation'})\r\n",
        "replacer.replace('congrats')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'congratulation'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnNX9HKPUvDq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}