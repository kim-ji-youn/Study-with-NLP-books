{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPwithPython1-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUdLgbq3LgdSpquRch1O4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim-ji-youn/Study-with-NLP-books/blob/main/1.%20Mastering%20Natural%20Language%20Processing%20with%20Python/1.%20NLPwithString/1.%20Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yYcVIHdG6_M"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTpHqwo9ICJc"
      },
      "source": [
        "## Sentence tokenization\r\n",
        "여러 개의 문장으로 구성되어 있는 문서를 각각의 문장으로 분리시켜주는 역할을 한다. \r\n",
        "* sent_tokenize  \r\n",
        "```\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "sent_tokenize(text)\r\n",
        "```\r\n",
        "* PunktSentenceTokenizer  \r\n",
        "```\r\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
        "tokenizer.tokenize(text)\r\n",
        "```\r\n",
        "* load ```pickle``` file : 외국어의 sentence tokenization\r\n",
        "```\r\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\r\n",
        "tokenizer.tokenize(text)\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlAV-KhlLvsM",
        "outputId": "c4b3713c-05ec-469c-f377-fb7c5a9cfbdc"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooMgtcVpCvaZ"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVQagr8EJ2Qr",
        "outputId": "3d5320ae-9950-4a18-e9e7-8874d9ff639c"
      },
      "source": [
        "# sent_tokenize\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "text = \"\"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\r\n",
        "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. Python is often described as a \"batteries included\" language due to its comprehensive standard library.\r\n",
        "\"\"\"\r\n",
        "sentences = sent_tokenize(text)\r\n",
        "print(\"total sentences are\", len(sentences), \"sentences\")\r\n",
        "for i, sent in enumerate(sentences) :\r\n",
        "  print(i+1, \"\\t\", sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total sentences are 6 sentences\n",
            "1 \t Python is an interpreted, high-level and general-purpose programming language.\n",
            "2 \t Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\n",
            "3 \t Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
            "4 \t Python is dynamically typed and garbage-collected.\n",
            "5 \t It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming.\n",
            "6 \t Python is often described as a \"batteries included\" language due to its comprehensive standard library.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9hw1IbKI0D8"
      },
      "source": [
        "## Word tokenization\r\n",
        "* word_tokenize\r\n",
        "  * 가장 일반적인 tokenizer\r\n",
        "```\r\n",
        "from nltk import word_tokenize\r\n",
        "word_tokenize(text)\r\n",
        "```\r\n",
        "* TreebankWokdTokenizer\r\n",
        "  * Penn Treebank Corpus에 따른 기준 사용\r\n",
        "  * 분리된 축약형 사용 ex) \"don't\" -> \"do\", \"n't\"\r\n",
        "```\r\n",
        "from nltk.tokenize import TreebankWordTokenizer\r\n",
        "tokenizer = TreebankWordTokenizer()\r\n",
        "tokenizer.tokenize(text)\r\n",
        "```\r\n",
        "* WordPunctTokenizer\r\n",
        "  * 분리된 문장 부호로 작동 ex) \"don't\" -> \"don\", \"'\", \"t\"\r\n",
        "  * 문장 부호를 완전히 새로운 토큰으로 분할하여 제공\r\n",
        "```\r\n",
        "from nltk.tokenize import WordPunctTokenizer\r\n",
        "tokenizer - WordPunctTokenizer()\r\n",
        "tokenizer.tokenize(text)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYoU_pLkJDK2",
        "outputId": "38249242-56b6-4f52-c924-6217ed211fbb"
      },
      "source": [
        "#word_tokenize\r\n",
        "from nltk import word_tokenize\r\n",
        "text = \"Don't hesitate to ask questions. Please ask me anything!\"\r\n",
        "word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Do',\n",
              " \"n't\",\n",
              " 'hesitate',\n",
              " 'to',\n",
              " 'ask',\n",
              " 'questions',\n",
              " '.',\n",
              " 'Please',\n",
              " 'ask',\n",
              " 'me',\n",
              " 'anything',\n",
              " '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n3J79i_U0dZ",
        "outputId": "f3054b60-5ae9-4b6e-b6a6-f6e90cb7775b"
      },
      "source": [
        "sentences = sent_tokenize(text)\r\n",
        "tokens = []\r\n",
        "num_tokens = 0\r\n",
        "for sent in sentences :\r\n",
        "  tokens.append(word_tokenize(sent))\r\n",
        "  num_tokens += len((word_tokenize(sent)))\r\n",
        "\r\n",
        "print(\"Total tokens are\", num_tokens, \"tokens\")\r\n",
        "for i, sent in enumerate(tokens):\r\n",
        "  print(f\"=== {i+1} sentence ===\")\r\n",
        "  for j, token in enumerate(tokens[i]):\r\n",
        "    print(j+1, \"\\t\", token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total tokens are 12 tokens\n",
            "=== 1 sentence ===\n",
            "1 \t Do\n",
            "2 \t n't\n",
            "3 \t hesitate\n",
            "4 \t to\n",
            "5 \t ask\n",
            "6 \t questions\n",
            "7 \t .\n",
            "=== 2 sentence ===\n",
            "1 \t Please\n",
            "2 \t ask\n",
            "3 \t me\n",
            "4 \t anything\n",
            "5 \t !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cJrwF2VPam",
        "outputId": "c4739ce9-c4f1-4be1-b3c1-a8dff87fe3dd"
      },
      "source": [
        "#TreebankWordTokenizer\r\n",
        "from nltk.tokenize import TreebankWordTokenizer\r\n",
        "tokenizer = TreebankWordTokenizer()\r\n",
        "tokens = tokenizer.tokenize(text)\r\n",
        "print(\"Total number of tokens: \", len(tokens))\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tokens:  11\n",
            "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions.', 'Please', 'ask', 'me', 'anything', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVjsBHZSYW9V",
        "outputId": "a8095efb-855f-4fbc-aaff-199f04c42c77"
      },
      "source": [
        "#WordPunctTokenizer\r\n",
        "from nltk.tokenize import WordPunctTokenizer\r\n",
        "tokenizer = WordPunctTokenizer()\r\n",
        "tokens = tokenizer.tokenize(text)\r\n",
        "print(\"Total number of tokens: \", len(tokens))\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tokens:  13\n",
            "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions', '.', 'Please', 'ask', 'me', 'anything', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3lqqdCNYz0Y"
      },
      "source": [
        "## 정규표현식(Regular expressions)을 사용한 Tokenization\r\n",
        "\r\n",
        "1. 클래스 활용 1: import RegexpTokenizer  \r\n",
        "1-1. 공백 + 특수 문자\r\n",
        "```\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\r\n",
        "tokenizer.tokenize(text)\r\n",
        "```\r\n",
        "1-2. 공백 단위\r\n",
        "```\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "tokenizer = RegexpTokenizer('\\s+', gaps = True)\r\n",
        "tokenizer.tokenize(text)\r\n",
        "```\r\n",
        "\r\n",
        "2. 함수 활용: import regexp_tokenize\r\n",
        "```\r\n",
        "from nltk.tokenize import regexp_tokenize\r\n",
        "print(regexp_tokenize(text, pattern = '\\w+|\\$[\\d\\.]+|\\S+'))\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxSyUZJiYiKa",
        "outputId": "0b1e41e3-2e75-4e7f-ab31-b1c096bb819c"
      },
      "source": [
        "# 공백 & 특수문자\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\r\n",
        "tokens = tokenizer.tokenize(text)\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', 't', 'hesitate', 'to', 'ask', 'questions', 'Please', 'ask', 'me', 'anything']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Foos6wleMVT",
        "outputId": "14dc175d-4468-4b76-d0fa-f62c3424646d"
      },
      "source": [
        "#공백\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "tokenizer = RegexpTokenizer('\\s+', gaps = True)\r\n",
        "tokens = tokenizer.tokenize(text)\r\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Don't\", 'hesitate', 'to', 'ask', 'questions.', 'Please', 'ask', 'me', 'anything!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjUzFYvqcnPc",
        "outputId": "f351ec1b-9490-450c-b195-ac8267f623a4"
      },
      "source": [
        "# 함수 사용\r\n",
        "from nltk.tokenize import regexp_tokenize\r\n",
        "print(regexp_tokenize(text, pattern = '\\w+|\\$[\\d\\.]+|\\S+'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions', '.', 'Please', 'ask', 'me', 'anything', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}